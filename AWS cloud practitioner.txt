https://www.udemy.com/course/practice-exams-aws-certified-cloud-practitioner/?couponCode=APR_24_GET_STARTED
    For exam practice
        (ALSO each chapter has questions html page at the end..)

==============================================================================================================
03 What is Cloud Computing?

    Examples of cloud computing types:
        - IaaS -> Amazon EC2, GCP, Digital Ocean
        - PaaS -> Elastic Bean Stack(AWS), Google App Engine
        - SaaS -> Rekognition for Machine learning on AWS, Dropbox, Zoom

    An AWS Region is a cluster of data centers
    A region has between 2 and 6 availability zones...each availability zone has redundant power, networking and
    connectivity..so they are isolated from disasters.

    AWS points of presence (edge locations) - these make content to be delivered to customers with lower latency

    Some services, like Route 53, IAM are Global...while other are regional (like EC2).
    NOT all AWS services are available in all the regions. You can see on their website if a specific service is available
    or not for a specific region.

!!!    Shared responsibility model
        This means that you as a developer are responsible for the security IN the cloud (customer data, app security,
        OS, firewall config, server side encryption etc) while AWS is responsible for the security OF the cloud...(the
        hardware, infrastructure, physical security)
            TODO Dan: question about this come in the exam


==============================================================================================================
04 IAM - Identity and Access Management

    We already used IAM when we created the root account. (also when I created accounts for the shopping app)

    Root account should not be used or shared...just to create other users

    Users can be grouped in a Group.
    Groups can only contain Users, not other Groups.
    A User can belong to multiple Groups

    Users and Groups can have json documents assigned which are called Policies

!!!!In AWS you apply the least privileged principle: don't give more permissions than the user needs.

    DanCurs     user created

    A Policy is a json document, that contains a version, and one or more Statements.
    The Statement element has several important parts:
        - effect : whether the Statement allows or denies access
        - principal : user/account/role for which the Statement applies
        - action : list of actions that are being allowed or denied
        - resource : list of resources to which the actions are applied

    Users on AWS console should use MFA (multi factor authentication)

    3 ways to connect to AWS:
        - AWS web console                       (protected by password and MFA)
        - AWS CLI (command line interface)      (PROTECTED by keys)
        - AWS SDK                               (PROTECTED by keys)

    Acess keys are generated from the AWS console.

    Installed AWS CLI on the machine :)
        "The AWS CLI first checks environment variables for credentials. If you've set the AWS_ACCESS_KEY_ID and
        AWS_SECRET_ACCESS_KEY environment variables, those will be used for authentication. You can check your
        system's environment variables to see if they're set."
            Dan: because i used this setup, when i worked in the past on the shop app, for uploading S3 pictures,
            now when i follow the tutorial, i created an AWS CLI profile, called "dancurs"..which needs to be specified
            each time i run an AWS CLI command, otherwise it will use the env variable key, which only has S3 permissions

    Example:
        aws iam list-users --profile dancurs

    An alternative to AWS CLI is AWS cloud shell.
        "AWS CloudShell is a browser-based shell that gives you command-line access to your AWS resources in the
        selected AWS region. AWS CloudShell comes pre-installed with popular tools for resource management and creation.
        You have the same credentials as you used to log in to the console"

        So since you are logged in the web console, no need to hassle with generating keys etc
    Example:
         aws iam list-users

    AWS Cloud shell offers space as well...so if you create files there etc...they will be "persisted"
        "Storage included 1 GB of storage free per AWS region"

    Some AWS services will need to perform actions on your behalf. To do so, we will assign permissions to AWS Services,
    with IAM Roles.
        Example if you start an EC2 instance (virtual server) and you want this to access some AWS service, like listing users,
        then you need to assign a role to EC2. They are also often used with Lambda.

    Auditing ..If you select a user and go to  the Access Advisor tab, you can see what AWS Services he accessed and when.
    This tool is very useful as if you notice that a user does not use some services, you can remove access for those services, and
    thus minimize the no of operations that he can perform.
    Similarly there is a Credentials Report, that lists all IAM users in your account, shows the status of access keys
    for each user, including:
    Access Key ID: The unique identifier for the access key. Status: Indicates whether the access key is active or inactive.
    Last Used Date: The date and time when the access key was last used for an API call (if applicable).


==============================================================================================================
05 EC2 - Elastic Compute Cloud

    This is Infrastructure as a service offering from Amazon
    It mainly consists of:
        - Renting Virtual Machines (EC2)
        - Storing data on virtual drives (EBS)
        - Distributing Load across Machines (ELB)

    Sizing and configuration options :
        - operating system : linux, windows, macos
        - CPU
        - RAM
        - storage :
            - network attached (EBS)
            - hardware (ECS instance store)
        - network card
        - firewall rules
        - bootstrap script, configure at first launch

    Bootstrap script allows you to                  (this script runs with root acess)
        - install updates
        - install software
        - download files from the internet
        - anything else you might think about

            Example used in the course:  (this will launch a webserver and write a html file to it)
#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

!!!!!
    In order to make the EC2 accessible via ssh and from the outside world, check out the youtube tutorial from the end
    (" AWS how to setup VPC, Public, Private Subnet, NAT, Internet Gateway, Route Table?")
    and the diagram "AWS VPC Subnet InternetGateway RouteTable.png" to better understand the networking elements.

    EC2 instance types
    https://aws.amazon.com/ec2/instance-types/
        General Purpose
            General purpose instances provide a balance of compute, memory and networking resources, and can be used
            for a variety of diverse workloads. These instances are ideal for applications that use these resources in
            equal proportions such as web servers and code repositories.
            Applications built on open-source software such as application servers, microservices, gaming servers,
            midsize data stores, and caching fleets.

        Compute Optimized
            Compute Optimized instances are ideal for compute bound applications that benefit from high performance
            processors. Instances belonging to this category are well suited for batch processing workloads, media
            transcoding, high performance web servers, high performance computing (HPC), scientific modeling, dedicated
            gaming servers and ad server engines, machine learning inference and other compute intensive applications.

        Memory Optimized
            Memory optimized instances are designed to deliver fast performance for workloads that process large data
            sets in memory.

        Accelerated Computing
            Accelerated computing instances use hardware accelerators, or co-processors, to perform functions, such as
            floating point number calculations, graphics processing, or data pattern matching, more efficiently than
            is possible in software running on CPUs.

        Storage Optimized
            Storage optimized instances are designed for workloads that require high, sequential read and write access
            to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency,
            random I/O operations per second (IOPS) to applications.
                Usecases:
                    SQL and NoSQL dbs
                    cache for in memory DB like Redis
                    data warehouse apps
                    distributed file systems

        HPC Optimized
            High performance computing (HPC) instances are purpose built to offer the best price performance for running
            HPC workloads at scale on AWS. HPC instances are ideal for applications that benefit from high-performance
            processors such as large, complex simulations and deep learning workloads.

    Security groups
        - are the fundamental of network security in AWS
        - they control how traffic is allowed into or out of our EC2 instances
        - security groups only contain allow rules !!!!!

        - they act as a firewall on our EC2 instances
        - one security group can be attached to multiple EC2 instances, and one EC2 instance can also have multiple
        security groups
        - all inbound traffic is BLOCKED by default
        - all outbound traffic is ALLOWED by default

        Dan: see "05- security groups diagram.png"

    Classic ports to know
        - 22 = SSH (secure shell) - log into linux instance
        - 21 = FTP - upload files
        - 22 = SFTP (secure file transfer protocol) - upload files using SSH
        - 80 = HTTP - access unsecure websites
        - 443 = HTTPS - access secure websites
        - 3389 = RDP (remote desktop protocol) - log into a Windows instance

        http://13.60.59.239/

    SSH to EC2 instance
        ssh -i EC2-Tutorial.pem ec2-user@ec2-13-60-59-239.eu-north-1.compute.amazonaws.com
            - you need to run this command from the secrets folder where the EC2-Tutorial.pem file contains a private key,
            that was either generated or selected during instance creation
            - ec2-user is  user created for linux that is running on the instance

        sudo -s (to become root)

!!! You should assign a role to the EC2 instance, by selecting the instance and going to Actions -> Security -> Modify IAM role.
    After doing the above step if you look into the EC2 instance settings ,in the Security tab you should see the IAM role.
    If you assign a role like "DemoRoleForEC2" that has "IAMReadOnlyAccess" permission, then in the ssh console you can
    run something like
        "aws iam list-users"

!!! Which purchasing option is better for me ?      (using a resort as an analogy for EC2)
        - On demand : coming and staying in a resort, whenever we like, we pay the full price.
        - Reserved : like planning ahead, and if we plan to stay for a long time (1-3 years), we may get a good discount
        - Savings plan : pay a certain amount per hour for certain period and stay in any room type(eg King Suite, Sea View)
        - Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms. You can
            get kicked out at any time. (so this is for jobs/processes that can handle being killed any time)
        - Dedicated hosts: we book an entire building of the resort
        - capacity reservations : you book a room for a period with full price, even if you do not stay in it

        Dan: see "5-2 price comparison.png"


==============================================================================================================
06 EC2 Instance Storage

    EBS Volume
     - means "elastic block store" is a network drive that you can attach to your instances while they run
     - allows you to persist instance data, even after their termination; this means that one can terminante an instance
     and after some time start a new instance, with the same EBS volume and have access to the same data that the previous
     EC2 instance had
     - an EBS can be mounted only for one EC2 instance at a time; however one EC2 instance can have multiple EBS volumes
     - they are bound to availability zones
     - think of them as a network USB stick !!!
     - the connection between EBS and EC2 is via a network which means that latency can occur
!!!  - when you create an EBS it has a flag "Delete on termination" ...which if set to true, when the EC2 is terminated,
        will be deleted
      - EBS snapshots are a backup of of your EBS volume at a moment in time
      - you can copy EBS snapshots across availability zones or regions
      - EBS snapshot archive is a storage that is 75% cheaper...but this means that it will take betweem 24 an 72 hours to
      retrieve that archive...so cheaper means slower :)
      - there is also a Recycle Bin feature for which you can set up rules as in how many days to keep there a delete snapshot,
      this is obviously useful in cases of accidental deletion

    AMI
        - amazon machine image
        - customization of an EC2 instance...you add your own software, configuration, operating system etc
        - this provides faster boot time because all your software is pre-packaged
        - are built for a specific region, and then they can be copied across regions
        - we can make and maintain our own AMI; you would create an EC2 instance, customize it how you want, then when
        ready, you stop it to maintain data integrity. Then you can build the AMI (this will also create EBS snapshots) and
        finally you can use that custom AMI
        - or you can buy AMIs from the marketplace

    EC2 image builder
        - used to automate the creation of Virtual machines or container images
        => automate the creation, maintain, validate and test EC2 AMIs
        - can be run on a schedule
        - the output of a builder can be an AMI or a Docker image
            Dan: mi-a dat eroare cand am urmat pasiii...aparent instanta de EC2 care ar fi trebuit sa ruleze procesul de
            creare al imaginii avea probleme...pe care nu am mai stat sa le identific..
        - cu DanPipe2 am reusit sa fac AMI... practic am lasat default la un pasul unde e "Create infrastructure
        configuration using service defaults" ...asta foloseste un EC2 instance care costa bani ...insa ideea e sa opresc
        EC2 dupa ce termina de creat imaginea

    EC2 Instance store
        - if you need high performance hard disk you must use EC2 instance store
        - better I/O performance
        - EC2 instance store lose their storage data if they are stopped...so this is suited for buffer / cache/ temporary
        data
        - for long term storage EBS is the choice

    EFS - Elastic file system
        - this is a managed NFS (network file system) that can be mounted on 100s of EC2 instances at a time, even if
        these EC2s are located in multiple availability zones
        - works with linux instances
        - a variant of this is EFS Infrequent Access (EFS-IA) , which is a storage class that is optimized for files not accessed
        every day (up to 92% lower costs)
        - EFS can be configured to automatically move files to EFS-IA after a certain period of time since they were last
        accessed

    Amazon FSx
        - use 3rd party high performance file systems on AWS (in case you do not like the offered storage options from
        AWS...or you want to store some data on your servers instead of Amazons for various reasons)
        -  Amazon FSx for windows file server
        -  Amazon FSx for Lustre (name is derived from "Linux" and "cluster")


==============================================================================================================
07 ELB & ASG - Elastic Load Balancing & Auto Scaling Groups

    - Vertical scalability means increasing the size of the instance...imagine a call center...and a Junior employee,
    vertical scaling would mean to replace him with a senior employee when there are too many calls
        so go from t3.micro to t3.large instance
    - there is a limit on how much you can scale this way : the hardware

    - Horizontal scalability ...means increasing the number of instances ...so in the call center example, this means
    adding more employees based on demand

    - High availability goes hand in hand with Horizontal scalability...and it means that your system runs in at least
    2 availability zones...in the call center example it means to have a call center in New York and another one in London

    - a load balancer is a server that forwards internet traffic to multiple servers (EC2 instances). By using one you can
    expose a single point of access (DNS) for your application
    - the load balancer will not direct traffic to a failling instance, because it performs regular health checks
    - you can use a load balancer across multiple availability zones

    - ELB (Elastic Load Balancer) is a managed load balancer...AWS guarantees that it will be working and takes care of
    upgrades, maintenance etc
    - AWS offers 3 kinds of ELBs:
        1. application load balancer (HTTP/HTTPS only)
        2. network load balancer (allows TCP...maybe for gaming, offer higher performance)
        3. classic load balancer (this offering is being retired by Amazon)

    Dan: see "AWS Load Balancer to 2 EC2 instances .png" to understand the elements needed to be configured for a load
        balancer...there is a youtube video about this below as well..

    - Auto Scaling groups are collections of Amazon EC2 instances that enable automatic scaling and fleet
    management features. These features help you maintain the health and availability of your applications.
    - you can choose various metrics, like CPU usage, and once the running EC2 instances reach that CPU usage threshold,
    new EC2 instances are created.
    - these new instances are added to the Target Group that is configured for the Load Balancer..


==============================================================================================================
08 S3

    - "infinitely scaling storage"
    - for example when you do an EBS snapshot, this is stored on S3

    Use cases
        - backup and storage
        - disaster recovery
        - archive
        - application and media storing
        - data lakes and big data analytics
        - software delivery
        - static website

    - allows users to store objects (files) into buckets (directories)
    - buckets must have a globally unique name, across all regions and accounts !!!!
    - buckets are defined at region level
    - naming convention for buckets
        - no uppercase
        - no underscore
        - 3-63 chars long
        - not ip
        - should start with lowecare or number

    - objects have a key.. an Amazon S3 object key acts like a unique identifier within a specific S3 bucket
        ex : for s3://my-bucket/my_file.txt the key is my_file.txt
                 s3://my-bucket/randomDirectory/my_file.txt the key is randomDirectory/my_file.txt
                 in this case the "randomDirectory" is called prefix

    - you can also add metadata (list of key-value pairs)
    - you can also add tag, (unicode key/value pair...up to 10..) useful for security and lifecycle
    - you can also have version id

    S3 Security
        - 1. IAM policies - which API calls should be allowed for a specific user from IAM console
        - 2. bucket policies - bucket wide rules from S3 console - allows cross account ...JSON based look similar to IAM policies
            - they specify the resources (buckets and objects and the set of API to allow or deny), the actions (set of
            API to allow or deny), the effect (allow or deny) and the principal (user or account to which the policy applies)
        - 3. Object Access Control List (ACL) - finer grain
        - 4. encryption - encrypt objects in S3 using encryption keys

    S3 Versioning
        - it is enabled at bucket level
        - same key overwrite will increment the version : 1, 2, 3 ...
        - it is a best practice to version your buckets:
            - protects against unwanted deletes
            - easy to roll back to previous version
        - suspending versioning does not delete old versions

    S3 Access logs
        - for audit purposes, you may want to log access to S3 buckets
        - any request, made to S3, FROM any account, authorized or unauthorized, will be logged into another S3 bucket
        - those logs can be analyzed with an analysis tool
        Dan: the logs will appear in the logs bucket after about an hour...so with delay...

    S3 Replication
        - 2 types, CRR (cross regional replication) and SRR (same region replication)
        - in order for this to work, you must enable versioning both in the source and the destination bucket
        - the copy process happens behind the scenes, and is asynchronous
        - we must give proper IAM permission to S3

    S3 Durability and Availability
        - durability : if you store 10 mil objects in S3, you can expect to lose 1 object once every 10.000 years; durability
        is the same for all storage classes (see below)
        - availability : how readily available a service is; this depends on the storage class...for example S3 standard has
        99.99% availability, which means not available 53 minutes per year

    S3 storage classes
        - S3 standard general purpose
            - 99.99% availability
        - S3 STANDARD infrequent access
            - lower cost than S3 standard..usecase would be backups / disaster recovery
            - 99.9% availability
        - S3 one zone infrequent access
            - high durability in one availability zone, but data is lost if availability zone is destroyed
            - uses cases would be secondary backups of on premise data, or data that you can recreate
        - S3 glacier instant retrieval
            - low cost object storage used for archive / backup             (applies to all glacier)
            - price is per storage + retrieval cost                          (applies to all glacier)
            - milliseccond retrieval
            - usecase...great for data that is accessed once per quarter for example
        - S3 glacier flexible retrieval
            - various longer retrieval times offered
        - S3 glacier deep archive
            - long storage.. lowest cost ....retrieval is in 12 hours +
        - S3 intelligent tiering
            - moves objects between tiers based on usage...for this extra functionality you pay a small fee, however
            there are no retrieval charges in S3 intelligent tiering

        Dan: when you upload a file you can specify the Storage Class..default one is Standard...also you can move objects
        to different classes depinding on needs

    S3 encryption
        - server side encryption means that the server, S3 in this case, encrypts the file after recieving it.
        - client side encryption means that the file is encrypted before uploading it

    AWS snow family
        - highly secure, portable devices, to collect and process data at the edge and migrate data into our out of AWS
        - this service is useful when dealing with a lot of data, since upload times over the internet can take too much
         time when dealing with TBs or PBs of data ...even exabytes EB (1 EB = 100O PB = 1_000_000 TB )

    AWS storage gateway
        - is a bridge between on premise data and cloud data in S3


==============================================================================================================
09 Databases & Analytics

    - If you want to structure data you will need a database
    - you can define relationships between datasets
    - various types of DBs
        - Relational Databases
        - NoSQL
            - NOT relational DBs
            - flexible schema..easier to evolve data model...data can be nested, support for arrays (think of a JSON document
            DB)
            - you can do horizontal scaling
            - examples : key-value DBs, document, graph, in memory

    - sure you can run a DB yourself on an EC2 instance, but in that case you need to manually handle things like resiliency,
    backups, patching, high availability, scaling, fault tolerance etc
    - in this chapter we go through all DBs offered by Amazon

    RDS ( Relational Database Service )
        - allows creation of DBs in the cloud that will be managed by AWS
        - various types of DBs: Postgres, MySql, Maria DB, Oracle, MS SQL server, Aurora (proprietary of AWS..not open sourced)
        - we have the possibility to add read replicas for our DB for improving the read performance
        - you can't SSH into your DB instance :(

        - Aurora is a cloud optimized DB and claims 5x performance improvement over MySQL running on RDS, and over 3x performance
        over Postgres running on RDS
            (Using Aurora locks you into the AWS ecosystem, making it harder to migrate to another cloud provider if needed.)
        - Aurora is not in the free tier...

    Amazon ElastiCache
        - ElastiCache is used to get a managed Redis or Memcached
        - caches are in memory DBs with high performance and low latency
        - this helps reduce load off databases for read intense workloads

    Dynamo DB
        - highly available DB with replication across 3 availability zones
        - it is a NoSql DB...a key value database
        - SCALES to massive workloads, and it is a serverless database
            (In essence, serverless databases offer a convenient and scalable way to manage your application's data
            without worrying about server infrastructure.)
        - in the hands on example you will see that you do not create a server/instance ...just a table ...
        - low latency retrieval

        - Dynamo DB Accelerator - aka DAX - is a fully managed in memory cache for DynamoDB... DAX offeres a 10x improvement
        when accessing data; difference between DAX and ElastiCache is that DAX is only for DynamoDB, while ElastiCache can
        be used for other DBs

    Redshift
        - based on PostgreSql, but it is not used for OLTP (online transaction processing)...this is what RDS is good for,
         but it is good for OLAP (online analytical and processing),...so good for using data warehousing and generating
         reports etc
        - data is loaded into it every hour, not every second
        - it has a feature called MPP (masively parallel query execution) which is used to perform the reporting computations
        very quickly

    Amazon EMR (elastic map reduce)
        - it is used to create Hadoop clusters (Big Data) to analyze and process large amounts of data
        - the clusters can be made of hundreds of EC2 instances
        - EMR takes care of provisioning and configuring all those EC2 instances to work together

    Amazon Athena
        - serverless query service to perform analytics against S3 objects
        - use standard SQL language to query the files

    Amazon QuickSight
        - serverless machine learning powered business intelligence service to create interactive dashboards
        - in order words, a tool for creating a dashboard over your data, that you can use to get insights into what is
        happening in your app
        - it is integrated with RDS, Aurora, Athena, Redshift, S3..

    Document DB
        - DocumentDB is the AWS implementation for MongoDB, just like Aurora is for the MySql/Postgres, which are relational
        DBs
        - so it is a NoSQL db
        - so it is compatible with MongoDB, which is used to store, query and index JSON data
        - similarly with Aurora, it is fully managed, highly available with replication over 3 availability zones

    Amazon Neptune
        - this is a graph DB

    Amazon QLDB
        - quantum ledger DB
        - a ledger is a book recording of financial transactions
        - fully managed, serverless, highly available, replication across 3 AZ
        - USED to review history of all the changes made to your application data over time
        - no entry can be removed or modified...data is cryptographically verifiable !!!!
        - although it sounds familiar to blockchain, this has a central authority component where data is stored ...so
        QLDB does not have descentralization

    Amazon Managed Blockchain
        - allows to build applications where multiple parties can execute transactions, without the need for a trusted,
        central authority...so it is descentralized
        - at the time of this course, this was compatible with the frameworks ethereum and hyperledger fabric

    DMS
        - database migration service
        - so this runs on a EC2 instance, and takes data from a source database and writes data to a target database
        - it supports homogeneous migrations, meaning from Oracle DB to Oracle DB
        - also supports heterogeneous migrations, for ex from MS Sql Server to Aurora

    AWS Glue
        - is a extract transform and load (ETL) service


==============================================================================================================
10 Other Compute Services_ ECS, Lambda, Batch, Lightsail

    ECS
        - Elastic Container Service
        - this is used to launch docker containers on AWS
        - you need to provision and maintain the infrastructure (the EC2 instances)

    Fargate
        - also used to launch docker containers on AWS
        - the difference from ECS is that with Fargate you do not need to provision the infrastructure...no EC2 instances to
        manage
        - AWS just runs the containers with the RAM and CPU that you need

    ECR
        - Elastic Container Registry
        - this is a private docker registry on AWS
        - this is where you need to store the AWS images in order to be able to run them on ECS or Fargate

    Serverless
        - means a new paradigm in which developers don't need to manage servers anymore...they just deploy the code
        - it was pioneered by AWS lambda, but now includes anything that is managed ...databases, messaging, storage..
        - it doesn't mean that there are no servers...it just means that developers don't need to manage / see the servers
        - examples : AWS S3, DynamoDB, Fargate...Lambda

    Lambda
        - so ...we talked about EC2...which is limited by RAM and CPU and runs continuously, even though we do not need
        it all the time
        - with Lambda, we just have virtual functions...that are limited by time...and they run on demand
        - scaling is automated !!!!
        - it is event-driven meaning that functions get called by AWS when needed.

    Amazon API Gateway
        - think of the example where you want to build a serverless API...a browser client makes REST calls...but these
        calls can't target Lambda directly...so they pass through the API Gateway, which in turn can call Lambda..which
        in turn can call Dynamo DB for example..
        - so API Gateway allows developers to easily create, publish, maintain, monitor and secure APIs

        Dan: see "10-1 API gateway usage.png"

    AWS Batch
        - a batch job is a job that has a start and end time
        - batch will dynamically launch EC2 instances or Spot instances
        - batch jobs run as Docker images

    Amazon Lightsail
        - it is a simpler alternative to using EC2, RDS, ELB, EBS, Route 53
        - great for people with little cloud experience...but it doesn't offer the full configurations available when you
        configure each of those services mentioned above for your app..


==============================================================================================================
11 Deployments & Managing Infrastructure at Scale

    Cloud Formation
        - is a declarative way (uses yaml or JSON format) of outlining your AWS infrastructure, for any resources
        - benefits of cloud formation:
            - infrastructure as code ...this means that you will not need to create resources manually from the WEB UI
            console; any modifications in the infrastructure can be reviewed through code review
            - costs : you get a resource identifier for each resource, so you can then easily track how much that resource
            costs you; also you can estimate the costs of your resources with CloudFormation template; also because Cloud
            Formation makes it easy to create AWS resources/services/instances, you can have costs saving...for example you can
            clean all the resources from AWS at 5pm, when developers leave work, and recreate them in morning at 8 AM
            automatically when the devs start working again
            - you don't need to reinvent the wheel...you can use existing Cloud Formation templates found online
            - you can generate diagrams of the resources used out of the template
        - in the web console there is an Application Composer, a web UI tool that one can use to create the diagram of
        the cloud, and generate the Cloud Formation configuration in either yaml or JSON

    Amazon Cloud Development Kit (CDK)
        - ALLOWS you to define the infrastructure using a programming language like Java
        - the code will eventually be translated into a Cloud Formation json/yaml file

    AWS Elastic Beanstalk
        - a developer centric view of deploying an application on AWS
        - THIS is a Platform as a Service (PaaS)
        - so you provide the jar/war , make some UI configs, and then the app is stated

    AWS Code deploy
        - a way to deploy code automatically
        - works with multiple EC2 instances, and with on premises servers (so AWS Code deploy is a hybrid service)

    AWS Code commit
        - is Amazons answer to GitHub ..
        - source control service that hosts git based repositories

    AWS Code build
        - builds code in cloud...compiles code, runs tests, creates packages
        - you only pay for build time...and it is fully managed, serverless

    AWS Code pipeline
        - orchestrate different steps needed to have code automatically pushed into production
        - code goes from Code Commit -> Code Build -> Code deploy -> Elastic Beanstalk

    AWS Code artifact
        - some sort of repository where artifacts can be stored (like Artifactory )

    AWS Code star
        - AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides
        a unified user interface, enabling you to easily manage your software development activities in one place.
        With AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start
        releasing code faster.

    AWS Cloud9
        - is a cloud IDE that you can use to edit run debug code in the cloud

    AWS systems manager (SSM)
        - get operational insights about your infrastructure
        - run commands across your fleet of servers
        - store parameter configuration with the SSM Parameter store
        - and many other managing functionalities


==============================================================================================================
12. Leveraging the AWS Global Infrastructure

    AWS Route 53
        - managed DNS ...a DNS is like a phone book, a collection of rules and records which help clients understand
        how to reach a server through URLs

            Dan: see "12-1 how a DNS works at a high level.png"

        - for exam you need to know the high level Routing policies:
            - simple routing policy: no healt checks ..browser calls Route 53 to get the IP...
            - weighted routing policy : Route 53 needs to select from several EC2 instances, and each of these has a different
                weight.. 70% 20% 10%..and this means that clients will be calling 70% of the time the first EC2 instance,
                20% the second one etc  (some kind of load balancing)
            - latency routing policy: if your app is distributed globally, and you have EC2 instances in California and
                Australia, and users all around the world, this policy will look at where the user is, and if he is close to
                California he will be redirected to those EC2s ...based on latency
            - failover routing policy: here Route 53 performs a health check on the EC2, which if it fails, it will redirect
                calls to another healthier EC2 instance

        - YOU HAVE the possibility to Register domains inside AWS Route 53
!!!!!   - 144 Route 53 Hands On.mp4
                Dan: very good tutorial of starting 2 EC2 instances in different regions, and registering them in Route 53
                    and by using a domain name, users get directed to the best EC2 instance
                    He used a vpn to simulate the fact that he is in another region of the world, so that the EC2 instance
                    closer to that region was serving requests.

    AWS Cloudfront
        - is a CDN
        - improves read performance as content is cached at edge
        - it offers DDOS protection , AWS Web App Firewall
        - it can cache from S3 buckets, Custom origin (HTTP) ..application load balancer, EC2 instance, or any http backend
        - great for static content that must be available everywhere

    AWS Global Accelerator
        - improves availability and performance by using the AWS global network
        - this works by having users connect to Edge locations, and then redirected to the resource through the AWS network
        - difference between this and CLoudfront is that CloudFront provides caching, while the Global Accelerator just
        passes the information through the AWS network for faster response times

    AWS Wavelength
        - bring AWS services to the edge of 5g networks..in order to provide low latency through the use of 5G networks

==============================================================================================================
13 Cloud Integrations

    TODO Continue from here


==============================================================================================================
xx


TODO Whenever you run into a problem with the course, maybe you can try and look over the videos of this other course:
    https://www.youtube.com/watch?v=bO25vbkoJlA&list=PL7iMyoQPMtAN4xl6oWzafqJebfay7K8KP

TODO AWS Certified Cloud Practitioner Certification Course (CLF-C02) - Pass the Exam!
    https://www.youtube.com/watch?v=NhDYbskXRgc&pp=ygUNYXdzIGxpZ2h0c2FpbA%3D%3D

TODO  Playlist about various AWS services
    https://www.youtube.com/watch?v=W6jQmVi31Xk&list=PLwyXYwu8kL0wg9R_VMeXy0JiK5_c70IrV

!!!  Dan: very good explanation of network settings needed in AWS. This fixed my issues with connecting to the EC2 inbound
        and outboud
            see "AWS VPC Subnet InternetGateway RouteTable.png"
    AWS how to setup VPC, Public, Private Subnet, NAT, Internet Gateway, Route Table?
        https://www.youtube.com/watch?v=43tIX7901Gs

    AWS ALB (Application Load Balancer) - Step By Step Tutorial (Part -9)               !!!! very good stuff
        https://www.youtube.com/watch?v=cuJTmBvFCS0

https://stackoverflow.com/questions/10253484/cant-access-site-on-ec2-instance-via-public-ip









































